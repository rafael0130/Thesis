{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafael0130/Thesis/blob/main/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01453173",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01453173",
        "outputId": "816041d1-0e40-4d3f-acc7-4096e5980aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# pip install mne\n",
        "# import mne\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.cElementTree as et\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "rootpath='/content/drive/MyDrive/thesis/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304c4fbd",
      "metadata": {
        "id": "304c4fbd"
      },
      "outputs": [],
      "source": [
        "def create_dataset(data, window_size,samp_rate):\n",
        "    windows = []\n",
        "    window_size=window_size*samp_rate\n",
        "    for i in range(0, data.shape[0], window_size):\n",
        "        window = data[i:i+window_size]\n",
        "        if len(window) == window_size:  # Discard incomplete windows (if any)\n",
        "            windows.append(window)\n",
        "    return np.array(windows)\n",
        "\n",
        "def create_mfccs(data):\n",
        "    data=data.reshape(data.shape[0],)\n",
        "    mfccs = librosa.feature.mfcc(y=data,n_mfcc =13,sr=500)\n",
        "    delta = librosa.feature.delta(mfccs)\n",
        "    delta2 = librosa.feature.delta(mfccs,order=2)\n",
        "    mfccs_concat = np.concatenate((mfccs,delta,delta2))\n",
        "    return mfccs_concat\n",
        "def get_labels(rootpath,patient_num,event_type,window_size,ch_len):\n",
        "  annotations =et.parse(rootpath+\"0000\"+patient_num+\"-100507.rml\").getroot()\n",
        "  events = []\n",
        "  # samp_rate=100\n",
        "  for event in annotations.iter('{http://www.respironics.com/PatientStudy.xsd}Event'):\n",
        "      events.append(event.attrib)\n",
        "  events_df= pd.DataFrame(events)\n",
        "  events_df.drop(['Machine', 'OriginatedOnDevice'], axis='columns', inplace=True)\n",
        "  events_df=events_df[events_df.Family==event_type].reset_index(drop=True)\n",
        "  events_df.Start=events_df.Start.astype('float64')\n",
        "  events_df.Duration=events_df.Duration.astype('float64')\n",
        "  events_df['end']=(events_df.Start+events_df.Duration)\n",
        "  events_df['window_start']=events_df.Start//window_size\n",
        "  events_df['window_end']=events_df.end//window_size\n",
        "  # print(events_df)\n",
        "  events_set=set(events_df.window_start)|set(events_df.window_end)\n",
        "  lbls=[]\n",
        "  for i in range(ch_len):\n",
        "      if i in events_set:\n",
        "          lbls.append(1)\n",
        "      else:\n",
        "          lbls.append(0)\n",
        "  return lbls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first=True\n",
        "window_size=30\n",
        "for num in ['0995','0999','1000']:\n",
        "  df = pd.read_csv(rootpath+'0000'+num+'-100507_Flow Patient-0.csv')\n",
        "  df2 = pd.read_csv(rootpath+'0000'+num+'-100507_SpO2.csv')\n",
        "  df3 = pd.read_csv(rootpath+'0000'+num+'-100507_Snore.csv')\n",
        "  ch1_temp = create_dataset(df,window_size,100)\n",
        "  ch2_temp = create_dataset(df2,window_size,1)\n",
        "  ch3_temp = create_dataset(df3,window_size,500)\n",
        "  ch_len=len(ch1_temp)\n",
        "  lbls_temp = get_labels(rootpath,num,\"Respiratory\",window_size,ch_len)\n",
        "  if first:\n",
        "    ch1=ch1_temp\n",
        "    ch2=ch2_temp\n",
        "    ch3=ch3_temp\n",
        "    lbls=lbls_temp\n",
        "    first=False\n",
        "  else:\n",
        "    ch1=np.concatenate((ch1,ch1_temp))\n",
        "    ch2=np.concatenate((ch2,ch2_temp))\n",
        "    ch3=np.concatenate((ch3,ch3_temp))\n",
        "    lbls=np.concatenate((lbls,lbls_temp))\n",
        "\n",
        "ch3_mod = []\n",
        "for item in range(len(ch3)):\n",
        "    result = create_mfccs(ch3[item]).reshape(39,30,1)\n",
        "    ch3_mod.append(result)\n",
        "#     plt.figure(figsize=(25,10))\n",
        "#     librosa.display.specshow(result,x_axis='time',sr=500)\n",
        "#     plt.colorbar(format='%+2f')\n",
        "ch3_mod=np.array(ch3_mod)"
      ],
      "metadata": {
        "id": "KxhNB_sdM-zf"
      },
      "id": "KxhNB_sdM-zf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe38ef18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe38ef18",
        "outputId": "32a1073b-8289-4938-a8dc-8c4747bc720c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1588, 3000, 1) (1588, 30, 1) (1588, 39, 30, 1) (1588,)\n"
          ]
        }
      ],
      "source": [
        "print(ch1.shape,ch2.shape,ch3_mod.shape,lbls.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(lbls.shape[0], size=lbls.shape[0])"
      ],
      "metadata": {
        "id": "g1iiXQO_UL6A"
      },
      "id": "g1iiXQO_UL6A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ch1=ch1[idx]\n",
        "ch2=ch2[idx]\n",
        "ch3_mod=ch3_mod[idx]\n",
        "lbls=lbls[idx]"
      ],
      "metadata": {
        "id": "fPgOb5ixUPth"
      },
      "id": "fPgOb5ixUPth",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_ratio = int(lbls.shape[0] * 0.8)\n",
        "split_ratio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58-JuBrDU6Yj",
        "outputId": "7705cb5e-77d0-41c1-875c-13217e7a84a1"
      },
      "id": "58-JuBrDU6Yj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1270"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f821cbfe",
      "metadata": {
        "id": "f821cbfe"
      },
      "outputs": [],
      "source": [
        "def encoder_1d(input_shape, modality_name,filters,code_size,l2_rate):\n",
        "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
        "    input = tf.keras.layers.Input(input_shape)\n",
        "    x = tf.keras.layers.Conv1D(filters=2 * filters,\n",
        "                               kernel_size=10,\n",
        "                               activation=\"linear\",\n",
        "                               padding=\"same\",\n",
        "                               strides=1,\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(l2_rate),\n",
        "                               kernel_initializer=initializer)(input)\n",
        "\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.PReLU(shared_axes=[1])(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=filters,\n",
        "                               kernel_size=8,\n",
        "                               activation=\"linear\",\n",
        "                               padding=\"same\",\n",
        "                               strides=1,\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(l2_rate),\n",
        "                               kernel_initializer=initializer)(x)\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.PReLU(shared_axes=[1])(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=code_size,\n",
        "                               kernel_size=4,\n",
        "                               activation=\"linear\",\n",
        "                               padding=\"same\",\n",
        "                               strides=1,\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(l2_rate),\n",
        "                               kernel_initializer=initializer)(x)\n",
        "    # output = tf.keras.layers.LayerNormalization()(x)\n",
        "    x = tf.keras.layers.PReLU(shared_axes=[1])(x)\n",
        "    output = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return tf.keras.models.Model(input, output, name=modality_name)\n",
        "\n",
        "def encoder_2d(input_shape, modality_name,filters):\n",
        "    input = tf.keras.layers.Input(input_shape) #input_shape=(height, width, channels)\n",
        "    # Add a 2D convolutional layer with 32 filters, 5x5 kernel size, and 'relu' activation\n",
        "    x = tf.keras.layers.Conv2D(filters, (5, 5), activation='relu')(input)\n",
        "\n",
        "    # Add a 2D max pooling layer with 2x2 pool size\n",
        "    x=tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Add a dropout layer with a dropout rate of 0.25 (adjust as needed)\n",
        "    x=tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Add a 2D convolutional layer with 32 filters, 5x5 kernel size, and 'relu' activation\n",
        "    x = tf.keras.layers.Conv2D(filters*2, (4, 4), activation='relu')(x)\n",
        "\n",
        "    # Add a 2D max pooling layer with 2x2 pool size\n",
        "    x=tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Add a dropout layer with a dropout rate of 0.25 (adjust as needed)\n",
        "    x=tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Add a 2D convolutional layer with 32 filters, 5x5 kernel size, and 'relu' activation\n",
        "    x = tf.keras.layers.Conv2D(filters*3, (3, 3), activation='relu')(x)\n",
        "\n",
        "    # Add a dropout layer with a dropout rate of 0.25 (adjust as needed)\n",
        "    output=tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "    return tf.keras.models.Model(input, output, name=modality_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993f29c3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "993f29c3",
        "outputId": "9291c3fa-20e7-4413-dafa-cad2b2c84140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_16\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_66 (InputLayer)       [(None, 3000, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " input_68 (InputLayer)       [(None, 30, 1)]              0         []                            \n",
            "                                                                                                  \n",
            " input_70 (InputLayer)       [(None, 39, 30, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " flow (Functional)           (None, 3000, 256)            36096     ['input_66[0][0]']            \n",
            "                                                                                                  \n",
            " spo2 (Functional)           (None, 30, 256)              36096     ['input_68[0][0]']            \n",
            "                                                                                                  \n",
            " snore (Functional)          (None, 5, 3, 108)            112572    ['input_70[0][0]']            \n",
            "                                                                                                  \n",
            " global_max_pooling1d_22 (G  (None, 256)                  0         ['flow[0][0]']                \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_23 (G  (None, 256)                  0         ['spo2[0][0]']                \n",
            " lobalMaxPooling1D)                                                                               \n",
            "                                                                                                  \n",
            " global_max_pooling2d_10 (G  (None, 108)                  0         ['snore[0][0]']               \n",
            " lobalMaxPooling2D)                                                                               \n",
            "                                                                                                  \n",
            " dense_45 (Dense)            (None, 256)                  65792     ['global_max_pooling1d_22[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_46 (Dense)            (None, 256)                  65792     ['global_max_pooling1d_23[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dense_47 (Dense)            (None, 256)                  27904     ['global_max_pooling2d_10[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 344252 (1.31 MB)\n",
            "Trainable params: 343228 (1.31 MB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# modality specific encoders\n",
        "mod_encoder = []\n",
        "mod_input = []\n",
        "\n",
        "input_shape = (ch1.shape[1], ch1.shape[2])\n",
        "encoder = encoder_1d(input_shape,\n",
        "                                  modality_name='flow',\n",
        "                                  filters=24,\n",
        "                                  code_size=256,\n",
        "                                  l2_rate=1e-4)\n",
        "\n",
        "\n",
        "mod_input.append(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "x_a = encoder(mod_input[-1])\n",
        "x_a = tf.keras.layers.GlobalMaxPooling1D()(x_a)\n",
        "x_a = tf.keras.layers.Dense(256, activation=\"linear\")(x_a)\n",
        "mod_encoder.append(x_a)\n",
        "\n",
        "input_shape = (ch2.shape[1], ch2.shape[2])\n",
        "encoder = encoder_1d(input_shape,\n",
        "                                  modality_name='spo2',\n",
        "                                  filters=24,\n",
        "                                  code_size=256,\n",
        "                                  l2_rate=1e-4)\n",
        "\n",
        "mod_input.append(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "x_a = encoder(mod_input[-1])\n",
        "x_a = tf.keras.layers.GlobalMaxPooling1D()(x_a)\n",
        "x_a = tf.keras.layers.Dense(256, activation=\"linear\")(x_a)\n",
        "mod_encoder.append(x_a)\n",
        "\n",
        "input_shape = (ch3_mod.shape[1], ch3_mod.shape[2],ch3_mod.shape[3])\n",
        "encoder = encoder_2d(input_shape,\n",
        "                                  modality_name='snore',\n",
        "                                  filters=36)\n",
        "\n",
        "mod_input.append(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "x_a = encoder(mod_input[-1])\n",
        "x_a = tf.keras.layers.GlobalMaxPooling2D()(x_a)\n",
        "x_a = tf.keras.layers.Dense(256, activation=\"linear\")(x_a)\n",
        "mod_encoder.append(x_a)\n",
        "\n",
        "embedding_model = tf.keras.Model(mod_input, mod_encoder)\n",
        "# input_x=embedding_model.input\n",
        "# print(input_x)\n",
        "# # input_x = ([(ch1.shape[1], ch1.shape[2]),(ch2.shape[1], ch2.shape[2])])\n",
        "# xi=embedding_model(input_x)\n",
        "# x = tf.keras.layers.Concatenate()(embedding_model.output)\n",
        "# x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "# x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "# # Final classification layer\n",
        "# output = tf.keras.layers.Dense(1, activation=binar)(x)\n",
        "\n",
        "# # Define the new model\n",
        "# classifier_model = tf.keras.models.Model(inputs=embedding_model.inputs, outputs=output)\n",
        "# classifier_model.summary()\n",
        "embedding_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3882d078",
      "metadata": {
        "id": "3882d078"
      },
      "outputs": [],
      "source": [
        "# X_train,X_test,y_train,y_test = [ch1[0:400,:,:],ch2[0:400,:,:]],[ch1[400:,:,:],ch2[400:,:,:]],lbls[0:400],lbls[400:]\n",
        "# # Step 3: Train Your Model\n",
        "# classifier_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# classifier_model.fit(X_train, y_train, epochs=10, batch_size=8)\n",
        "\n",
        "# # Evaluate the model on the test set\n",
        "# test_loss, test_accuracy = classifier_model.evaluate(X_test, y_test)\n",
        "# print(f'Test accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486b4a56",
      "metadata": {
        "id": "486b4a56"
      },
      "source": [
        "## Contrastive loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9bb170",
      "metadata": {
        "id": "cd9bb170"
      },
      "outputs": [],
      "source": [
        "def cocoa_loss(ytrue, ypred):\n",
        "    temperature = 0.5\n",
        "    lambd=3.9e-3\n",
        "    scale_loss=1/32\n",
        "    batch_size, dim_size = ypred.shape[1], ypred.shape[0]\n",
        "    print(ypred.shape)\n",
        "    # Positive Pairs\n",
        "    pos_error = []\n",
        "    for i in range(batch_size):\n",
        "        sim = tf.linalg.matmul(ypred[:, i, :], ypred[:, i, :], transpose_b=True)\n",
        "        sim = tf.subtract(tf.ones([dim_size, dim_size], dtype=tf.dtypes.float32), sim)\n",
        "        sim = tf.exp(sim/temperature)\n",
        "        pos_error.append(tf.reduce_mean(sim))\n",
        "    # Negative pairs\n",
        "    neg_error = 0\n",
        "    for i in range(dim_size):\n",
        "        sim = tf.cast(tf.linalg.matmul(ypred[i], ypred[i], transpose_b=True), dtype=tf.dtypes.float32)\n",
        "        sim = tf.exp(sim /temperature)\n",
        "        # sim = tf.add(sim, tf.ones([batch_size, batch_size]))\n",
        "        tri_mask = np.ones(batch_size ** 2, dtype=bool).reshape(batch_size, batch_size)\n",
        "        tri_mask[np.diag_indices(batch_size)] = False\n",
        "        off_diag_sim = tf.reshape(tf.boolean_mask(sim, tri_mask), [batch_size, batch_size - 1])\n",
        "        neg_error += (tf.reduce_mean(off_diag_sim, axis=-1))\n",
        "\n",
        "    error = tf.multiply(tf.reduce_sum(pos_error),scale_loss) + lambd * tf.reduce_sum(neg_error)\n",
        "\n",
        "    return error\n",
        "\n",
        "# ------------------------------------------------------------------------- #\n",
        "class DotProduct(tf.keras.layers.Layer):\n",
        "    def call(self, x, y):\n",
        "        x = tf.nn.l2_normalize(x, axis=-1)\n",
        "        y = tf.nn.l2_normalize(y, axis=-1)\n",
        "        return tf.linalg.matmul(x, y, transpose_b=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------- #\n",
        "class ContrastiveModel(tf.keras.Model):\n",
        "    def __init__(self, embedding_model, loss_fn, temperature=1.0, **kwargs):\n",
        "        super().__init__()\n",
        "        self.embedding_model = embedding_model\n",
        "        self._temperature = temperature\n",
        "        self._similarity_layer = DotProduct()\n",
        "        self._lossfn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            modality_embeddings = self.embedding_model(data, training=True)\n",
        "\n",
        "            sparse_labels = tf.range(tf.shape(modality_embeddings[0])[0])\n",
        "\n",
        "            pred = modality_embeddings\n",
        "            pred = tf.nn.l2_normalize(tf.stack(pred), axis=-1)\n",
        "\n",
        "            loss = self.compiled_loss(sparse_labels, pred)\n",
        "            loss += sum(self.losses)\n",
        "\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def call(self, input):\n",
        "        return self.embedding_model(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50fd1ba",
      "metadata": {
        "id": "e50fd1ba"
      },
      "outputs": [],
      "source": [
        "ssl = ContrastiveModel(embedding_model, cocoa_loss)\n",
        "batch_size=8\n",
        "train=tf.data.Dataset.from_tensor_slices((ch1, ch2, ch3_mod))\n",
        "X_train= train.batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79c7dc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b79c7dc1",
        "outputId": "e86042c1-313d-4516-a5fa-5f26a64f5e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "(3, 8, 256)\n",
            "(3, 8, 256)\n",
            "198/198 [==============================] - 107s 500ms/step - loss: 0.8226\n",
            "Epoch 2/10\n",
            "198/198 [==============================] - 91s 458ms/step - loss: 0.8195\n",
            "Epoch 3/10\n",
            "198/198 [==============================] - 93s 472ms/step - loss: 0.8198\n",
            "Epoch 4/10\n",
            "198/198 [==============================] - 93s 467ms/step - loss: 0.8188\n",
            "Epoch 5/10\n",
            "198/198 [==============================] - 93s 468ms/step - loss: 0.8182\n",
            "Epoch 6/10\n",
            "198/198 [==============================] - 91s 459ms/step - loss: 0.8179\n",
            "Epoch 7/10\n",
            "198/198 [==============================] - 113s 572ms/step - loss: 0.8174\n",
            "Epoch 8/10\n",
            "198/198 [==============================] - 95s 478ms/step - loss: 0.8175\n",
            "Epoch 9/10\n",
            "198/198 [==============================] - 98s 493ms/step - loss: 0.8184\n",
            "Epoch 10/10\n",
            "198/198 [==============================] - 101s 509ms/step - loss: 0.8180\n"
          ]
        }
      ],
      "source": [
        "# X_train,X_test,y_train,y_test = [ch1[0:500,:,:],ch2[0:500,:,:],ch3_mod[0:500,:,:]],[ch1[400:,:,:],ch2[400:,:,:],ch3_mod[400:,:,:]],[lbls[0:400],lbls[0:400],lbls[0:400]],[lbls[400:],lbls[400:],lbls[400:]]\n",
        "# Step 3: Train Your Model\n",
        "\n",
        "ssl.compile(optimizer='adam', loss=cocoa_loss, metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "result = ssl.fit(X_train, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60fdda81",
      "metadata": {
        "id": "60fdda81"
      },
      "source": [
        "## add the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb824977",
      "metadata": {
        "id": "cb824977"
      },
      "outputs": [],
      "source": [
        "base_model = ssl.embedding_model\n",
        "# for layer in base_model.layers:\n",
        "#     print(layer)\n",
        "#     layer.trainable = False\n",
        "input_x = base_model.input\n",
        "xi = base_model(input_x)\n",
        "x = tf.keras.layers.Concatenate()(xi)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.L1(0.005))(x)\n",
        "\n",
        "classifier_model = tf.keras.layers.Dense(1,activation=\"sigmoid\",kernel_regularizer=tf.keras.regularizers.L1(0.005),)(x)\n",
        "c_model = tf.keras.Model(input_x, classifier_model)\n",
        "c_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75552df2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75552df2",
        "outputId": "66c06b46-0380-4ac9-ceb6-97c22f32cd49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_66 (InputLayer)       [(None, 3000, 1)]            0         []                            \n",
            "                                                                                                  \n",
            " input_68 (InputLayer)       [(None, 30, 1)]              0         []                            \n",
            "                                                                                                  \n",
            " input_70 (InputLayer)       [(None, 39, 30, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " model_16 (Functional)       [(None, 256),                344252    ['input_66[0][0]',            \n",
            "                              (None, 256),                           'input_68[0][0]',            \n",
            "                              (None, 256)]                           'input_70[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate  (None, 768)                  0         ['model_16[0][0]',            \n",
            " )                                                                   'model_16[0][1]',            \n",
            "                                                                     'model_16[0][2]']            \n",
            "                                                                                                  \n",
            " flatten_7 (Flatten)         (None, 768)                  0         ['concatenate_7[0][0]']       \n",
            "                                                                                                  \n",
            " dense_48 (Dense)            (None, 128)                  98432     ['flatten_7[0][0]']           \n",
            "                                                                                                  \n",
            " dense_49 (Dense)            (None, 1)                    129       ['dense_48[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 442813 (1.69 MB)\n",
            "Trainable params: 441789 (1.69 MB)\n",
            "Non-trainable params: 1024 (4.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "c_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d40f82e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d40f82e",
        "outputId": "8de5a58b-94a1-408e-8074-fcf2baab0386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "158/158 [==============================] - 82s 480ms/step - loss: 20.7187 - accuracy: 0.5150\n",
            "Epoch 2/10\n",
            "158/158 [==============================] - 75s 471ms/step - loss: 12.1772 - accuracy: 0.5435\n",
            "Epoch 3/10\n",
            "158/158 [==============================] - 72s 456ms/step - loss: 8.8145 - accuracy: 0.5506\n",
            "Epoch 4/10\n",
            "158/158 [==============================] - 72s 458ms/step - loss: 7.0224 - accuracy: 0.5799\n",
            "Epoch 5/10\n",
            "158/158 [==============================] - 74s 470ms/step - loss: 5.9729 - accuracy: 0.5965\n",
            "Epoch 6/10\n",
            "158/158 [==============================] - 77s 486ms/step - loss: 5.2445 - accuracy: 0.6290\n",
            "Epoch 7/10\n",
            "158/158 [==============================] - 79s 499ms/step - loss: 4.7293 - accuracy: 0.6392\n",
            "Epoch 8/10\n",
            "158/158 [==============================] - 72s 455ms/step - loss: 4.3089 - accuracy: 0.6717\n",
            "Epoch 9/10\n",
            "158/158 [==============================] - 72s 453ms/step - loss: 3.9809 - accuracy: 0.6725\n",
            "Epoch 10/10\n",
            "158/158 [==============================] - 75s 473ms/step - loss: 3.6746 - accuracy: 0.7009\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c23028217e0>"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "train,val=tf.data.Dataset.from_tensor_slices((ch1[0:split_ratio], ch2[0:split_ratio], ch3_mod[0:split_ratio])),\\\n",
        "          tf.data.Dataset.from_tensor_slices((ch1[split_ratio:], ch2[split_ratio:], ch3_mod[split_ratio:]))\n",
        "train_lbl,val_lbl =  tf.data.Dataset.from_tensor_slices(lbls[0:split_ratio]),\\\n",
        "                     tf.data.Dataset.from_tensor_slices(lbls[split_ratio:])\n",
        "train=tf.data.Dataset.zip(train,train_lbl)\n",
        "val=tf.data.Dataset.zip(val,val_lbl)\n",
        "training= train.batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation = val.batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "c_model.fit(training, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = c_model.evaluate(validation)\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSlMNCUAgHXZ",
        "outputId": "ba4bc1bf-3f71-437a-9b26-12e7b6912698"
      },
      "id": "fSlMNCUAgHXZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39/39 [==============================] - 5s 106ms/step - loss: 3.6010 - accuracy: 0.6346\n",
            "Test accuracy: 0.6346153616905212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c880453d",
      "metadata": {
        "id": "c880453d",
        "outputId": "5dc1fc44-13a9-48da-a520-996917a9e25d"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Input \u001b[1;32mIn [112]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my_train2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "y_train2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0c676b",
      "metadata": {
        "id": "ee0c676b",
        "outputId": "4d9822e4-784b-458c-a3eb-814a03c89ee5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(400, 3000, 1)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train[0].shape\n",
        "# y_train[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7283d0e1",
      "metadata": {
        "id": "7283d0e1",
        "outputId": "f278514d-5cba-41b1-c253-fdf40c538d77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(96, 3000, 1)"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create TensorFlow Datasets from your data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(([X_train_modality_1, X_train_modality_2], y_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(([X_test_modality_1, X_test_modality_2], y_test)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Compile the model\n",
        "classifier_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "classifier_model.fit(train_dataset, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = classifier_model.evaluate(test_dataset)\n",
        "print(f'Test accuracy: {test_accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f2f1bbb",
      "metadata": {
        "id": "8f2f1bbb"
      },
      "outputs": [],
      "source": [
        "# class CustomLoss:\n",
        "#     # pos: exponential for positive example\n",
        "#     # neg: sum of exponentials for negative examples\n",
        "#     # N : number of negative examples\n",
        "#     # temperature : temperature scaling\n",
        "#     # tau : class probability\n",
        "#     def __init__(\n",
        "#             self,\n",
        "#             temperature=0.5,\n",
        "#             tau=0.01,\n",
        "#             beta=2,\n",
        "#             elimination_th=0,\n",
        "#             elimination_topk=0.1,\n",
        "#             lambd = 3.9e-3,\n",
        "#             scale_loss= 1/32,\n",
        "#             attraction=False\n",
        "#     ):\n",
        "#         self.temperature = temperature,\n",
        "#         self.tau = tau,\n",
        "#         self.beta = beta,\n",
        "#         self.elimination_th = elimination_th,\n",
        "#         self.elimination_topk = elimination_topk,\n",
        "#         self.attraction = attraction\n",
        "#         self.lambd = lambd\n",
        "#         self.scale_loss = scale_loss\n",
        "#         # Please double-check `reduction` parameter\n",
        "#         self.criterion = tf.keras.losses.BinaryCrossentropy(\n",
        "#             from_logits=False, reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "\n",
        "\n",
        "#     def get_loss_fn(self, loss_type):\n",
        "#         loss = None\n",
        "#         # Info-NCE\n",
        "#         if loss_type == \"nce\":\n",
        "#             def loss(ytrue, ypred):\n",
        "#                 all_sim = K.exp(ypred / self.temperature)\n",
        "#                 logits = tf.divide(\n",
        "#                     tf.linalg.tensor_diag_part(all_sim), K.sum(all_sim, axis=1))\n",
        "#                 print(logits)\n",
        "#                 lbl = np.ones(ypred.shape[0])\n",
        "#                 error = self.criterion(y_pred=logits, y_true=lbl)\n",
        "#                 return error\n",
        "\n",
        "#         # Debiased Contrastive Learning\n",
        "#         elif loss_type in [\"dcl\", \"harddcl\"]:\n",
        "#             def loss(ytrue, ypred):\n",
        "#                 # dcl: from Debiased Contrastive Learning paper: https://github.com/chingyaoc/DCL/\n",
        "#                 # harddcl: from ICLR2021 paper: Contrastive LEarning with Hard Negative Samples\n",
        "#                 # https://www.groundai.com/project/contrastive-learning-with-hard-negative-samples\n",
        "#                 # reweight = (beta * neg) / neg.mean()\n",
        "#                 # Neg = max((-N * tau_plus * pos + reweight * neg).sum() / (1 - tau_plus), e ** (-1 / t))\n",
        "#                 # hard_loss = -log(pos.sum() / (pos.sum() + Neg))\n",
        "#                 N = ypred.shape[0]\n",
        "#                 all_sim = K.exp(ypred / self.temperature)\n",
        "#                 pos_sim = tf.linalg.tensor_diag_part(all_sim)\n",
        "\n",
        "#                 tri_mask = np.ones(N ** 2, dtype=np.bool).reshape(N, N)\n",
        "#                 tri_mask[np.diag_indices(N)] = False\n",
        "#                 neg_sim = tf.reshape(tf.boolean_mask(all_sim, tri_mask), [N, N - 1])\n",
        "\n",
        "#                 reweight = 1.0\n",
        "#                 if loss_type == \"harddcl\":\n",
        "#                     reweight = (self.beta * neg_sim) / tf.reshape(tf.reduce_mean(neg_sim, axis=1), [-1, 1])\n",
        "#                 if self.beta == 0:\n",
        "#                     reweight = 1.0\n",
        "\n",
        "#                 Ng = tf.divide(\n",
        "#                     tf.multiply(self.tau[0] * (1 - N), pos_sim) + K.sum(tf.multiply(reweight, neg_sim), axis=-1),\n",
        "#                     (1 - self.tau[0]))\n",
        "#                 print(Ng)\n",
        "#                 # constrain (optional)\n",
        "#                 Ng = tf.clip_by_value(Ng, clip_value_min=(N - 1) * np.e ** (-1 / self.temperature[0]),\n",
        "#                                       clip_value_max=tf.float32.max)\n",
        "#                 error = K.mean(- tf.math.log(pos_sim / (pos_sim + Ng)))\n",
        "#                 return error\n",
        "\n",
        "#         elif loss_type == \"barlow\":\n",
        "#             def loss(ytrue, ypred):\n",
        "#                 # Barlow Twins: Self-supervised representation learning with redundancy reduction\n",
        "#                 # https://arxiv.org/pdf/2103.03230.pdf\n",
        "#                 N = ypred.shape[0]\n",
        "\n",
        "#                 # empirical cross-correlation matrix\n",
        "#                 corr = tf.divide(ypred,N)\n",
        "#                 on_diag = tf.linalg.tensor_diag_part(corr) # pos_sim\n",
        "#                 invariance = tf.multiply(K.sum(tf.math.pow(tf.math.subtract(on_diag,1),2)), self.scale_loss)\n",
        "\n",
        "#                 tri_mask = np.ones(N ** 2, dtype=np.bool).reshape(N, N)\n",
        "#                 tri_mask[np.diag_indices(N)] = False\n",
        "#                 off_diag = tf.reshape(tf.boolean_mask(corr, tri_mask), [N, N - 1])\n",
        "#                 redundancy = tf.multiply(tf.math.reduce_sum(tf.math.pow(off_diag,2)), self.scale_loss)\n",
        "\n",
        "#                 error = invariance + self.lambd * redundancy\n",
        "\n",
        "#                 return error\n",
        "#         # Contrasting More than two dimenstions\n",
        "#         elif loss_type == \"cocoa\":\n",
        "#             def loss(ytrue, ypred):\n",
        "#                 batch_size, dim_size = ypred.shape[1], ypred.shape[0]\n",
        "\n",
        "#                 # Positive Pairs\n",
        "#                 pos_error=[]\n",
        "#                 for i in range(batch_size):\n",
        "#                     sim = tf.exp(tf.linalg.matmul(ypred[:,i,:], ypred[:,i,:], transpose_b=True)/self.temperature)\n",
        "#                     tri_mask = np.ones(dim_size ** 2, dtype=np.bool).reshape(dim_size, dim_size)\n",
        "#                     tri_mask[np.diag_indices(dim_size)] = False\n",
        "#                     off_diag_sim = tf.reshape(tf.boolean_mask(sim, tri_mask), [dim_size, dim_size - 1])\n",
        "#                     pos_error.append(tf.reduce_sum(off_diag_sim))\n",
        "#                 # Negative pairs\n",
        "#                 neg_error = 0\n",
        "#                 for i in range(dim_size):\n",
        "#                     sim = tf.cast(tf.linalg.matmul(ypred[i], ypred[i], transpose_b=True), dtype=tf.dtypes.float32)\n",
        "#                     sim = tf.exp(sim/self.temperature)\n",
        "#                     tri_mask = np.ones(batch_size ** 2, dtype=bool).reshape(batch_size, batch_size)\n",
        "#                     tri_mask[np.diag_indices(batch_size)] = False\n",
        "#                     off_diag_sim = tf.reshape(tf.boolean_mask(sim, tri_mask), [batch_size, batch_size - 1])\n",
        "#                     neg_error += (tf.reduce_mean(off_diag_sim, axis=-1))\n",
        "\n",
        "#                 logits = tf.divide(pos_error, pos_error+neg_error)\n",
        "#                 lbl = np.ones(batch_size)\n",
        "#                 error = self.criterion(y_pred=logits, y_true=lbl)\n",
        "#                 return error\n",
        "\n",
        "#         elif loss_type == \"cocoa2\":\n",
        "#             def loss(ytrue, ypred):\n",
        "#                 batch_size, dim_size = ypred.shape[1], ypred.shape[0]\n",
        "#                 # Positive Pairs\n",
        "#                 pos_error = []\n",
        "#                 for i in range(batch_size):\n",
        "#                     sim = tf.linalg.matmul(ypred[:, i, :], ypred[:, i, :], transpose_b=True)\n",
        "#                     sim = tf.subtract(tf.ones([dim_size, dim_size], dtype=tf.dtypes.float32), sim)\n",
        "#                     sim = tf.exp(sim/self.temperature)\n",
        "#                     pos_error.append(tf.reduce_mean(sim))\n",
        "#                 # Negative pairs\n",
        "#                 neg_error = 0\n",
        "#                 for i in range(dim_size):\n",
        "#                     sim = tf.cast(tf.linalg.matmul(ypred[i], ypred[i], transpose_b=True), dtype=tf.dtypes.float32)\n",
        "#                     sim = tf.exp(sim / self.temperature)\n",
        "#                     # sim = tf.add(sim, tf.ones([batch_size, batch_size]))\n",
        "#                     tri_mask = np.ones(batch_size ** 2, dtype=bool).reshape(batch_size, batch_size)\n",
        "#                     tri_mask[np.diag_indices(batch_size)] = False\n",
        "#                     off_diag_sim = tf.reshape(tf.boolean_mask(sim, tri_mask), [batch_size, batch_size - 1])\n",
        "#                     neg_error += (tf.reduce_mean(off_diag_sim, axis=-1))\n",
        "\n",
        "#                 error = tf.multiply(tf.reduce_sum(pos_error),self.scale_loss) + self.lambd * tf.reduce_sum(neg_error)\n",
        "\n",
        "#                 return error\n",
        "\n",
        "#         elif loss_type == \"mse\":\n",
        "#             def loss(ytrue, ypred):\n",
        "#                 reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(ytrue, ypred)))\n",
        "#                 return reconstruction_error\n",
        "\n",
        "#         else:\n",
        "#             raise ValueError(\"Undefined loss function.\")\n",
        "\n",
        "#         return loss\n",
        "\n",
        "\n",
        "# def mse_loss(model, original):\n",
        "#   reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
        "#   return reconstruction_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a435a3b0",
      "metadata": {
        "id": "a435a3b0"
      },
      "outputs": [],
      "source": [
        "# custom_loss_obj = CustomLoss(temperature=(1/32))\n",
        "# loss_fn = custom_loss_obj.get_loss_fn(\"cocoa2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "109a63c2",
      "metadata": {
        "id": "109a63c2",
        "outputId": "acd1ad14-763a-4a19-de4f-8662822591ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.cocoa_loss(ytrue, ypred)>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cocoa_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265ab865",
      "metadata": {
        "id": "265ab865",
        "outputId": "7f4a10a3-f14e-488e-e079-6d5c471a5c56"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'loss_fn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mloss_fn\u001b[49m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'loss_fn' is not defined"
          ]
        }
      ],
      "source": [
        "loss_fn"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}